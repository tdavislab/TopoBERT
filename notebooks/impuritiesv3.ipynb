{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kmapper as km\n",
    "import kneed\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "import collections\n",
    "from dataclasses import dataclass\n",
    "import ast\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from diskcache import Cache\n",
    "from scipy.stats import entropy\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.special import kl_div\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "sns.set()\n",
    "cache = Cache('./cachev3/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 589\n",
    "EPOCH_GAP = 15\n",
    "LAYERS = [0, 1, 2]\n",
    "DATA = 'train'\n",
    "DATASET = 'bert-tiny'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    iteration: int = 0\n",
    "    layer: int = 0\n",
    "    metric: str = 'euclidean'\n",
    "    filter_func: str = 'l2'\n",
    "    intervals: int = 50\n",
    "    overlap: float = 0.5\n",
    "    min_samples: int = 5\n",
    "\n",
    "\n",
    "def read_labels(path):\n",
    "    label_data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            word_info, word_label = line.strip().split('\\t')\n",
    "            sent_info, word = word_info.split(':')\n",
    "            sent_info = ast.literal_eval(sent_info)\n",
    "            label_data.append([sent_info[0], sent_info[1], word, word_label])\n",
    "\n",
    "    return pd.DataFrame(label_data, columns=['sent_id', 'word_id', 'word', 'label'])\n",
    "\n",
    "\n",
    "def add_node_metadata(graph, metadata_source, activations):\n",
    "    # create PCA model first\n",
    "    nodewise_activations = np.vstack([np.mean(activations.iloc[graph['nodes'][node_name]], axis=0) for node_name in graph['nodes']])\n",
    "\n",
    "    for i, node_name in enumerate(graph['nodes']):\n",
    "        member_list = graph['nodes'][node_name]\n",
    "\n",
    "        metadata = [metadata_source.loc[member_index].tolist() for member_index in member_list]\n",
    "        graph['nodes'][node_name] = {'membership_ids': member_list, 'metadata': metadata,\n",
    "                                     'l2avg': np.average(metadata_source.loc[member_list]['l2norm']),\n",
    "                                     'type': 'train'}\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def elbow_eps(data):\n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    distances = np.sort(distances, axis=0)[::-1]\n",
    "    kneedle = kneed.KneeLocator(distances[:, 1], np.linspace(0, 1, num=len(distances)), curve='convex', direction='decreasing')\n",
    "    eps = kneedle.knee * 0.75 if kneedle.knee else 5\n",
    "    return eps\n",
    "\n",
    "\n",
    "def serialize_graph(graph):\n",
    "    nx_graph = km.adapter.to_networkx(graph)\n",
    "    js_graph = json_graph.node_link_data(nx_graph)\n",
    "\n",
    "    for i, node in enumerate(js_graph['nodes']):\n",
    "        js_graph['nodes'][i]['name'] = js_graph['nodes'][i]['id']\n",
    "        js_graph['nodes'][i]['l2avg'] = js_graph['nodes'][i]['membership']['l2avg']\n",
    "\n",
    "    for i, link in enumerate(js_graph['links']):\n",
    "        id_s = link['source']\n",
    "        id_t = link['target']\n",
    "        mem1 = [x['membership']['membership_ids'] for x in js_graph['nodes'] if x['id'] == id_s][0]\n",
    "        mem2 = [x['membership']['membership_ids'] for x in js_graph['nodes'] if x['id'] == id_t][0]\n",
    "        mem1, mem2 = set(mem1), set(mem2)\n",
    "        jaccard = len(mem1.intersection(mem2)) / len(mem1.union(mem2))\n",
    "        js_graph['links'][i]['intersection'] = jaccard\n",
    "\n",
    "    return js_graph\n",
    "\n",
    "\n",
    "def get_mapper(activations, labels, conf):\n",
    "    labels['l2norm'] = np.expand_dims(np.linalg.norm(activations.to_numpy(), axis=1), 1)\n",
    "    mapper = km.KeplerMapper()\n",
    "\n",
    "    if conf.filter_func == 'l1':\n",
    "        projected_data = np.linalg.norm(activations, ord=1, axis=1).reshape((activations.shape[0], 1))\n",
    "    elif conf.filter_func == 'l2':\n",
    "        projected_data = mapper.fit_transform(activations, projection='l2norm')\n",
    "    elif conf.filter_func == 'knn5':\n",
    "        projected_data = mapper.fit_transform(activations, projection='knn_distance_5') / 5\n",
    "    else:\n",
    "        raise KeyError('Unexpected filter function')\n",
    "\n",
    "    eps = elbow_eps(activations)\n",
    "    graph = mapper.map(projected_data, activations, clusterer=DBSCAN(eps=eps, metric=conf.metric, min_samples=conf.min_samples),\n",
    "                       cover=km.Cover(n_cubes=conf.intervals, perc_overlap=conf.overlap))\n",
    "\n",
    "    add_node_metadata(graph, labels, activations)\n",
    "\n",
    "    return serialize_graph(graph)\n",
    "\n",
    "\n",
    "def node_purity(node, label):\n",
    "    metadata = node['membership']['metadata']\n",
    "    label_counts = collections.Counter([x[3] for x in metadata])\n",
    "    return (label_counts[label], len(metadata), label)\n",
    "\n",
    "\n",
    "def compute_purities(graph):\n",
    "    point_node_purities = collections.defaultdict(list)\n",
    "\n",
    "    for node in graph['nodes']:\n",
    "        metadata = node['membership']['metadata']\n",
    "\n",
    "        for i, point_id in enumerate(node['membership']['membership_ids']):\n",
    "            point_node_purities[point_id].append((i, node_purity(node, metadata[i][3])))\n",
    "\n",
    "    return point_node_purities\n",
    "\n",
    "\n",
    "def get_activations_labels(activation_file, label_file):\n",
    "    activations = pd.read_csv(activation_file, delim_whitespace=True, header=None)\n",
    "    labels = read_labels(label_file)\n",
    "    return activations, labels\n",
    "\n",
    "\n",
    "def get_graph(config: Config, caching=True):\n",
    "    layer = config.layer\n",
    "\n",
    "    ACTIVATION_FILE = f'../backend/data/{DATASET}/fine-tuned-bert-base-uncased/{DATA}/{config.iteration}/{layer}.txt'\n",
    "    label_file = f'../backend/data/{DATASET}/entities/{DATA}.txt'\n",
    "\n",
    "    cache_key = f'{ACTIVATION_FILE}-{label_file}-{config}'\n",
    "\n",
    "    if caching and cache_key in cache:\n",
    "        graph = cache[cache_key]\n",
    "    else:\n",
    "        activations, labels = get_activations_labels(ACTIVATION_FILE, label_file)\n",
    "        graph = get_mapper(activations, labels, config)\n",
    "        cache[cache_key] = graph\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def density(X):\n",
    "    X_score = np.arange(-0.1, 1.1, 0.01)\n",
    "    # densities = np.exp(KernelDensity(kernel='gaussian', bandwidth=0.001).fit(np.array(X).reshape(-1, 1)).score_samples(X_score.reshape(-1, 1)))\n",
    "    densities = gaussian_kde(np.array(X)).evaluate(X_score)\n",
    "    # threshold = 1e-10\n",
    "    # densities[densities < threshold] = 0\n",
    "    return densities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nodewise_purities(graphs, method='entropy'):\n",
    "    entropy_bound = entropy(np.array([1] * len(labels)) / len(labels))\n",
    "\n",
    "    def node_purities(graph):\n",
    "        purities = []\n",
    "        for node in graph['nodes']:\n",
    "            n_purity = [0] * len(labels)\n",
    "            metadata = node['membership']['metadata']\n",
    "            label_counts = collections.Counter([x[3] for x in metadata])\n",
    "\n",
    "            for label, count in label_counts.items():\n",
    "                n_purity[label_dict[label]] = count / len(metadata)\n",
    "\n",
    "            if method == 'entropy':\n",
    "                purities.append(1 - entropy(n_purity) / entropy_bound)\n",
    "            elif method == 'invclass':\n",
    "                purities.append(1 / len(label_counts))\n",
    "            else:\n",
    "                raise ValueError('Unexpected purity method')\n",
    "\n",
    "        return purities\n",
    "\n",
    "    nodewise_purities = []\n",
    "\n",
    "    for iteration, graph in enumerate(graphs):\n",
    "        nodewise_purities.append(node_purities(graph))\n",
    "\n",
    "    df_nodewise_purities = pd.DataFrame([[x * EPOCH_GAP, y] for x, y in enumerate(nodewise_purities)])\n",
    "    df_nodewise_purities.columns = ['iteration', 'purity']\n",
    "\n",
    "    return df_nodewise_purities\n",
    "\n",
    "\n",
    "def compute_pointwise_purities(graphs):\n",
    "\n",
    "    def point_purities(graph):\n",
    "        purities = []\n",
    "        for node in graph['nodes']:\n",
    "            metadata = node['membership']['metadata']\n",
    "            label_counts = collections.Counter([x[3] for x in metadata])\n",
    "\n",
    "            for sent_id, word_id, word, label, norm in metadata:\n",
    "                purities.append(label_counts[label] / len(metadata))\n",
    "\n",
    "            # purities.extend([label_counts[label] / len(metadata) for label in labels])\n",
    "        return purities\n",
    "\n",
    "    pointwise_purities = []\n",
    "\n",
    "    for iteration, graph in enumerate(graphs):\n",
    "        pointwise_purities.append(point_purities(graph))\n",
    "\n",
    "    df_pointwise_purities = pd.DataFrame([[x * EPOCH_GAP, y] for x, y in enumerate(pointwise_purities)])\n",
    "    df_pointwise_purities.columns = ['iteration', 'purity']\n",
    "\n",
    "    return df_pointwise_purities\n",
    "    return pointwise_purities\n",
    "\n",
    "\n",
    "def plot_ridge_plots(data_df, title_col, accessor_col, layer, title_text=None):\n",
    "    fig, axes = plt.subplots(nrows=len(data_df), ncols=1, figsize=(8, 15), sharex=True)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        title = data_df.iloc[i][title_col]\n",
    "        row_data = data_df.iloc[i][accessor_col]\n",
    "\n",
    "        sns.kdeplot(row_data, ax=ax, shade=True)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel(title, rotation=0)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "\n",
    "    if title_text is not None:\n",
    "        fig.suptitle(title_text)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'images/purity_dists_v3/nodewise_entropy_{DATASET}_layer{layer}_{DATA}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def plot_kljs(purity_df, layer):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
    "\n",
    "    purity_kdes = np.vstack(purity_df.purity.apply(density))\n",
    "\n",
    "    kl_divs = kl_div(purity_kdes, purity_kdes[0])\n",
    "    axes[0].plot(np.arange(0, NUM_EPOCHS, EPOCH_GAP), np.sum(kl_divs, axis=1))\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('KL-divergence')\n",
    "    axes[0].set_title('KL-divergence of purities w.r.t Epoch 0')\n",
    "\n",
    "    # plt.figure()\n",
    "    jen_shannon = []\n",
    "    for i in range(len(purity_kdes)):\n",
    "        jen_shannon.append(jensenshannon(purity_kdes[0], purity_kdes[i]))\n",
    "\n",
    "    axes[1].plot(np.arange(0, NUM_EPOCHS, EPOCH_GAP), jen_shannon)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Jensen-Shannon')\n",
    "    axes[1].set_title('Jensen-Shannon distance of purities w.r.t Epoch 0')\n",
    "\n",
    "    plt.savefig(f'images/purity_dists_v3/kl_js_distances_{DATASET}_layer{layer}_{DATA}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f045ff16094434088104a02a1a68fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217a8941a7134fae8e821ff1d46ad921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0089309d9fb64835b0d25eb44d218c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\archi\\anaconda3\\envs\\probing\\lib\\site-packages\\kneed\\knee_locator.py:304: UserWarning: No knee/elbow found\n",
      "  warnings.warn(\"No knee/elbow found\")\n"
     ]
    }
   ],
   "source": [
    "for layer in LAYERS:\n",
    "    graphs = []\n",
    "\n",
    "    for fileindex in tqdm(range(0, NUM_EPOCHS, EPOCH_GAP)):\n",
    "        config = Config(iteration=fileindex, layer=layer, metric='euclidean', filter_func='l2', intervals=50, overlap=0.5, min_samples=3)\n",
    "        graphs.append(get_graph(config, caching=True))\n",
    "\n",
    "    labels = sorted(read_labels(f'../backend/data/{DATASET}/entities/{DATA}.txt').label.unique().tolist())\n",
    "    # convert label to dict with positions as value\n",
    "    label_dict = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "    nodewise_purities = compute_nodewise_purities(graphs, method='entropy')\n",
    "    plot_ridge_plots(nodewise_purities, 'iteration', 'purity', layer, title_text=f'Layer {layer}')\n",
    "    plot_kljs(nodewise_purities, layer)\n",
    "\n",
    "    # nodewise_purities = compute_nodewise_purities(graphs, method='invclass')\n",
    "    # plot_ridge_plots(nodewise_purities, 'iteration', 'purity')\n",
    "    # plt.savefig(f'images/purity_dists_v3/nodewise_invclass_{DATASET}_layer{LAYER}_{DATA}.png', dpi=300)\n",
    "\n",
    "    # pointwise_purities = compute_pointwise_purities(graphs)\n",
    "    # plot_ridge_plots(pointwise_purities, 'iteration', 'purity')\n",
    "    # plt.savefig(f'images/purity_dists_v3/pointwise_{DATASET}_layer{LAYER}_{DATA}.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc5676d886a2778973ff3e34aec4cad8965d07457120715c016f8e27d3c13f9b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('probing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
