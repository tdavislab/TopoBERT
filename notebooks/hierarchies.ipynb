{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, to_tree\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import kneed\n",
    "from collections import deque, Counter\n",
    "from scipy.spatial.distance import pdist\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sys.setrecursionlimit(2000)\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(path, dataset='ss-role'):\n",
    "    label_data = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if dataset in ['ss-func', 'ss-role']:\n",
    "                word_info, word_label = line.strip().split('\\t')\n",
    "                sent_info, word = word_info.split(':')\n",
    "                sent_info = ast.literal_eval(sent_info)\n",
    "                label_data.append([sent_info[0], sent_info[1], word, word_label])\n",
    "            elif dataset == 'dep':\n",
    "                word_pair, word_label = line.strip().split('\\t')\n",
    "                word_pair = word_pair.split('--')\n",
    "                if len(word_pair) == 2:\n",
    "                    word1, word2 = word_pair\n",
    "                elif len(word_pair) == 3:\n",
    "                    word1, word2 = word_pair[0], '--'\n",
    "                sent_info = [0, 0]\n",
    "                label_data.append([sent_info[0], sent_info[1], f'{word1}--{word2}', word_label])\n",
    "            else:\n",
    "                raise ValueError('Dataset not supported')\n",
    "\n",
    "    return pd.DataFrame(label_data, columns=['sent_id', 'word_id', 'word', 'label'])\n",
    "\n",
    "\n",
    "def get_graph(dataset, dist_metric, filter, intervals, overlap, iteration, layer, datasplit):\n",
    "    # make request to local server at port 5000 at \\graph\n",
    "    # with the query: {params: 'ss-role_euclidean_l2_50_50', iteration: 0, layer: 12, datasplit: 'train'}\n",
    "    # and save the response as a variable\n",
    "    r = requests.get(\n",
    "        'http://localhost:5000/graph',\n",
    "        params={'params': f'{dataset}_{dist_metric}_{filter}_{intervals}_{overlap}', 'iteration': iteration, 'layer': layer,\n",
    "                'datasplit': datasplit})\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    graph = nx.json_graph.node_link_graph(data['graph'])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_activations(dataset, iteration, layer, datasplit):\n",
    "    activations = pd.read_csv(\n",
    "        f'../backend/data/{dataset}/fine-tuned-bert-base-uncased/{datasplit}/{iteration}/{layer}.txt', delim_whitespace=True, header=None)\n",
    "    labels = read_labels(f'../backend/data/{dataset}/entities/{datasplit}.txt', dataset=dataset)\n",
    "\n",
    "    return activations, labels\n",
    "\n",
    "\n",
    "def point_to_node(nodes, num_points=4282):\n",
    "    ptn_dict = {}\n",
    "    for point_idx in range(num_points):\n",
    "        for node_idx, node in enumerate(nodes):\n",
    "            node_data = nodes[node]['membership']['membership_ids']\n",
    "\n",
    "            if point_idx in node_data:\n",
    "                ptn_dict[point_idx] = node_idx\n",
    "\n",
    "    return ptn_dict\n",
    "\n",
    "\n",
    "def linkage_matrix(model):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "\n",
    "    return linkage_matrix\n",
    "\n",
    "\n",
    "def elbow_eps(data):\n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    distances = np.sort(distances, axis=0)[::-1]\n",
    "    kneedle = kneed.KneeLocator(distances[:, 1], np.linspace(0, 1, num=len(distances)), curve='convex', direction='decreasing')\n",
    "    eps = kneedle.knee * 0.75\n",
    "    return eps\n",
    "\n",
    "\n",
    "def populate_tree_labels(treeNode, labels):\n",
    "    if treeNode.id < len(labels):\n",
    "        treeNode.label = set([labels.iloc[treeNode.id]['label'].split('.')[1]])\n",
    "    else:\n",
    "        # popluate children\n",
    "        populate_tree_labels(treeNode.left, labels)\n",
    "        populate_tree_labels(treeNode.right, labels)\n",
    "\n",
    "        # label is union of labels of left and right children\n",
    "        treeNode.label = treeNode.left.label | treeNode.right.label\n",
    "\n",
    "    return treeNode\n",
    "\n",
    "\n",
    "def populate_tree_labels_counter(treeNode, labels):\n",
    "    if treeNode.id < len(labels):\n",
    "        treeNode.label = Counter([labels.iloc[treeNode.id]['label'].split('.')[1]])\n",
    "    else:\n",
    "        # popluate children\n",
    "        populate_tree_labels_counter(treeNode.left, labels)\n",
    "        populate_tree_labels_counter(treeNode.right, labels)\n",
    "\n",
    "        # label is union of labels of left and right children\n",
    "        treeNode.label = treeNode.left.label + treeNode.right.label\n",
    "\n",
    "    return treeNode\n",
    "\n",
    "\n",
    "def process_label(label, max_length=15):\n",
    "    label_str = ','.join(sorted(label))\n",
    "    if len(label_str) > max_length:\n",
    "        return f'{label_str[:max_length]}... ({len(label)})'\n",
    "    else:\n",
    "        return label_str\n",
    "\n",
    "\n",
    "def process_label_counter(label, max_length=15):\n",
    "    label_str = ','.join([k for k, v in label.most_common()])\n",
    "    if len(label_str) > max_length:\n",
    "        return f'{label_str[:max_length]}... ({len(label)})'\n",
    "    else:\n",
    "        return label_str\n",
    "\n",
    "\n",
    "def bfs_traversal(treeNode, graph, max_level=5):\n",
    "    q = deque()\n",
    "\n",
    "    q.append(treeNode)\n",
    "    level = 0\n",
    "\n",
    "    while len(q) > 0 and level < max_level:\n",
    "        level_size = len(q)\n",
    "\n",
    "        for _ in range(level_size):\n",
    "            node = q.popleft()\n",
    "            num_labels = len(node.label)\n",
    "\n",
    "            graph.add_node(node.id, label=process_label_counter(node.label), class_counts=node.label)\n",
    "\n",
    "            # terminate if node's label has one label\n",
    "            if num_labels > 1:\n",
    "                if node.left is not None:\n",
    "                    q.append(node.left)\n",
    "                    graph.add_edge(node.id, node.left.id, weight=node.left.dist)\n",
    "                    graph.add_node(node.left.id, label=process_label(node.left.label), class_counts=node.left.label)\n",
    "\n",
    "                if node.right is not None:\n",
    "                    q.append(node.right)\n",
    "                    graph.add_edge(node.id, node.right.id, weight=node.right.dist)\n",
    "                    graph.add_node(node.right.id, label=process_label(node.right.label), class_counts=node.right.label)\n",
    "\n",
    "        level += 1\n",
    "\n",
    "\n",
    "def node_to_point_matrix(activations, ptn_dict, node_dist_matrix):\n",
    "    point_dist_mat_from_graph = np.zeros((len(activations), len(activations)))\n",
    "\n",
    "    # populate point_dist_mat_from_graph\n",
    "    for point_idx1 in range(len(activations)):\n",
    "        for point_idx2 in range(len(activations)):\n",
    "            if point_idx1 not in ptn_dict or point_idx2 not in ptn_dict:\n",
    "                point_dist_mat_from_graph[point_idx1][point_idx2] = 100\n",
    "            elif point_idx1 != point_idx2:\n",
    "                point_dist_mat_from_graph[point_idx1, point_idx2] = node_dist_matrix[ptn_dict[point_idx1], ptn_dict[point_idx2]]\n",
    "\n",
    "    return point_dist_mat_from_graph\n",
    "\n",
    "\n",
    "def find_pointwise_parents(point_cloud_size, graph):\n",
    "    point_to_node = dict()\n",
    "    graph_nodes = graph.nodes\n",
    "\n",
    "    for point_idx in range(point_cloud_size):\n",
    "        for node_idx, node in enumerate(graph_nodes):\n",
    "            node_data = graph_nodes[node]['membership']['membership_ids']\n",
    "\n",
    "            if point_idx in node_data:\n",
    "                point_to_node[point_idx] = [node_idx] + point_to_node.get(node_idx, [])\n",
    "\n",
    "    # convert each value to set\n",
    "    for k, v in point_to_node.items():\n",
    "        point_to_node[k] = set(v)\n",
    "\n",
    "    return point_to_node\n",
    "\n",
    "\n",
    "def point_distance_piecewise(graph, activations, pointwise_parents, node_distances):\n",
    "    pointwise_distances = np.zeros((len(activations), len(activations)))\n",
    "\n",
    "    # max_distance = np.ma.masked_invalid(node_distances).max()\n",
    "    max_dist = pdist(activations.iloc[np.random.randint(activations.shape[0], size=500), :]).max() + 5\n",
    "\n",
    "    for point_idx1 in tqdm(range(len(activations))):\n",
    "        for point_idx2 in range(point_idx1 + 1, len(activations)):\n",
    "            # euclidean_dist_p1p2 = np.linalg.norm(activations.iloc[point_idx1, :] - activations.iloc[point_idx2, :])\n",
    "\n",
    "            if point_idx1 != point_idx2:\n",
    "                parent1 = pointwise_parents.get(point_idx1, None)\n",
    "                parent2 = pointwise_parents.get(point_idx2, None)\n",
    "\n",
    "                if parent1 is None or parent2 is None:          # if either point has no parent, set distance to 2*max\n",
    "                    pointwise_distances[point_idx1, point_idx2] = max_dist * 2\n",
    "                elif len(parent1) == len(parent2) == 1:\n",
    "                    p1, p2 = list(parent1)[0], list(parent2)[0]\n",
    "                    if p1 != p2:                                # if both points have single and distinct parents \n",
    "                        pointwise_distances[point_idx1][point_idx2] = np.min([node_distances[p1, p2], max_dist])\n",
    "                    else:                                       # if both points have a single same parent\n",
    "                        pointwise_distances[point_idx1][point_idx2] = 0\n",
    "                else:                                           # if both points have multiple parents, compute all possible distances                    \n",
    "                    distances = []\n",
    "                    for p1 in parent1:\n",
    "                        for p2 in parent2:\n",
    "                            if p1 != p2:\n",
    "                                distances.append(np.min([node_distances[p1, p2], max_dist]))\n",
    "                            else:\n",
    "                                distances.append(0)\n",
    "\n",
    "                    pointwise_distances[point_idx1][point_idx2] = np.min(distances) \n",
    "\n",
    "            # set pointwise distance of mirror coordinates\n",
    "            pointwise_distances[point_idx2][point_idx1] = pointwise_distances[point_idx1][point_idx2]               \n",
    "\n",
    "    return pointwise_distances\n",
    "\n",
    "\n",
    "def compute_pointwise_distances(graph, activations, labels):\n",
    "    # compute pairwise distances of the graph nodes\n",
    "    node_distances = nx.algorithms.shortest_paths.dense.floyd_warshall_numpy(graph, weight='centroid_dist')\n",
    "\n",
    "    # compute parent node indices for each point\n",
    "    pointwise_parents = find_pointwise_parents(len(activations), graph)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = get_graph('ss-role', 'euclidean', filter='l2', intervals=50, overlap=50, iteration=175, layer=12, datasplit='train')\n",
    "# activations, labels = get_activations('ss-role', 175, 12, 'train')\n",
    "\n",
    "# node_distances = nx.algorithms.shortest_paths.dense.floyd_warshall_numpy(graph, weight='centroid_dist')\n",
    "# pointwise_parents = find_pointwise_parents(len(activations), graph)\n",
    "# pointwise_distances = point_distance_piecewise(graph, activations, pointwise_parents, node_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4282/4282 [01:16<00:00, 56.22it/s] \n",
      "100%|██████████| 4282/4282 [03:02<00:00, 23.52it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x1f8561e9ec8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def induced_hierarchy2(dataset, iteration, layer, datasplit, max_level=10, dist_metric='euclidean', filter='l2', intervals=50, overlap=50):\n",
    "    filename = f'images/hierarchies2/{dataset}_{datasplit}_iter{iteration}_layer{layer}_level{max_level}.svg'\n",
    "\n",
    "    # if os.path.isfile(filename):\n",
    "    #     print('File exists, returning')\n",
    "    #     return\n",
    "\n",
    "    # get mapper graph\n",
    "    graph = get_graph(dataset, dist_metric, filter=filter, intervals=intervals,\n",
    "                      overlap=overlap, iteration=iteration, layer=layer, datasplit=datasplit)\n",
    "    # graph = get_graph(DATASET, 'euclidean', filter='l2', intervals=50, overlap=50, iteration=ITERATION, layer=LAYER, datasplit='train')\n",
    "\n",
    "    activations, labels = get_activations(dataset, iteration, layer, datasplit)\n",
    "\n",
    "    node_distances = nx.algorithms.shortest_paths.dense.floyd_warshall_numpy(graph, weight='centroid_dist')\n",
    "    pointwise_parents = find_pointwise_parents(len(activations), graph)\n",
    "    point_dist_mat_from_graph = point_distance_piecewise(graph, activations, pointwise_parents, node_distances)\n",
    "\n",
    "    # Perform hierarchical clustering using pointwise distance matrix\n",
    "    model_aggclust_mapper = AgglomerativeClustering(n_clusters=None, distance_threshold=99, affinity='precomputed', linkage='average')\n",
    "    model_aggclust_mapper.fit(point_dist_mat_from_graph)\n",
    "\n",
    "    # compute the linkage matrix\n",
    "    linkage_matrix_mapper = linkage_matrix(model_aggclust_mapper)\n",
    "    tree = to_tree(linkage_matrix_mapper)\n",
    "    populate_tree_labels_counter(tree, labels)\n",
    "\n",
    "    # Create a networkx directional graph\n",
    "    linkage_graph = nx.DiGraph()\n",
    "\n",
    "    # Populate the graph with using BFS traversal of the tree\n",
    "    bfs_traversal(tree, linkage_graph, max_level=max_level)\n",
    "\n",
    "    # Save SVG file of the graph\n",
    "    Ag = nx.nx_agraph.to_agraph(linkage_graph)\n",
    "    Ag.layout(prog='neato', args=\"-Nshape=box\")\n",
    "    Ag.draw(filename, format='svg')\n",
    "\n",
    "    return linkage_graph\n",
    "\n",
    "# induced_hierarchy2('ss-role', iteration=0, layer=12, datasplit='train', max_level=25)\n",
    "induced_hierarchy2('ss-role', iteration=5, layer=12, datasplit='train', max_level=25)\n",
    "# induced_hierarchy2('ss-role', iteration=65, layer=12, datasplit='train', max_level=25)\n",
    "induced_hierarchy2('ss-role', iteration=175, layer=12, datasplit='train', max_level=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x1f856c8a908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def induced_hierarchy3(dataset, iteration, layer, datasplit, max_level=10, dist_metric='euclidean', filter='l2', intervals=50, overlap=50):\n",
    "    filename = f'images/hierarchies3/euc_{dataset}_{datasplit}_iter{iteration}_layer{layer}_level{max_level}.svg'\n",
    "\n",
    "    # if os.path.isfile(filename):\n",
    "    #     print('File exists, returning')\n",
    "    #     return\n",
    "\n",
    "    # get mapper graph\n",
    "    # graph = get_graph(dataset, dist_metric, filter=filter, intervals=intervals,\n",
    "    #                   overlap=overlap, iteration=iteration, layer=layer, datasplit=datasplit)\n",
    "    # graph = get_graph(DATASET, 'euclidean', filter='l2', intervals=50, overlap=50, iteration=ITERATION, layer=LAYER, datasplit='train')\n",
    "\n",
    "    activations, labels = get_activations(dataset, iteration, layer, datasplit)\n",
    "\n",
    "    # node_distances = nx.algorithms.shortest_paths.dense.floyd_warshall_numpy(graph, weight='centroid_dist')\n",
    "    # pointwise_parents = find_pointwise_parents(len(activations), graph)\n",
    "    # point_dist_mat_from_graph = point_distance_piecewise(graph, activations, pointwise_parents, node_distances)\n",
    "\n",
    "    # Perform hierarchical clustering using pointwise distance matrix\n",
    "    model_aggclust_mapper = AgglomerativeClustering(n_clusters=None, distance_threshold=1000, linkage='average')\n",
    "    model_aggclust_mapper.fit(activations)\n",
    "\n",
    "    # compute the linkage matrix\n",
    "    linkage_matrix_mapper = linkage_matrix(model_aggclust_mapper)\n",
    "    tree = to_tree(linkage_matrix_mapper)\n",
    "    populate_tree_labels_counter(tree, labels)\n",
    "\n",
    "    # Create a networkx directional graph\n",
    "    linkage_graph = nx.DiGraph()\n",
    "\n",
    "    # Populate the graph with using BFS traversal of the tree\n",
    "    bfs_traversal(tree, linkage_graph, max_level=max_level)\n",
    "\n",
    "    # Save SVG file of the graph\n",
    "    Ag = nx.nx_agraph.to_agraph(linkage_graph)\n",
    "    Ag.layout(prog='neato', args=\"-Nshape=box\")\n",
    "    Ag.draw(filename, format='svg')\n",
    "\n",
    "    return linkage_graph\n",
    "\n",
    "induced_hierarchy3('ss-role', iteration=5, layer=12, datasplit='train', max_level=25)\n",
    "# induced_hierarchy2('ss-role', iteration=65, layer=12, datasplit='train', max_level=25)\n",
    "induced_hierarchy3('ss-role', iteration=175, layer=12, datasplit='train', max_level=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-57db6179c210>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;31m# # Train Layer 12\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minduced_hierarchy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ss-role'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m175\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;31m# induced_hierarchy('ss-role', iteration=5, layer=12, datasplit='train', max_level=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-57db6179c210>\u001b[0m in \u001b[0;36minduced_hierarchy\u001b[1;34m(dataset, iteration, layer, datasplit, max_level, dist_metric, filter, intervals, overlap)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# get mapper graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     graph = get_graph(dataset, dist_metric, filter=filter, intervals=intervals,\n\u001b[1;32m---> 10\u001b[1;33m                       overlap=overlap, iteration=iteration, layer=layer, datasplit=datasplit)\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# graph = get_graph(DATASET, 'euclidean', filter='l2', intervals=50, overlap=50, iteration=ITERATION, layer=LAYER, datasplit='train')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-9f0d168c892a>\u001b[0m in \u001b[0;36mget_graph\u001b[1;34m(dataset, dist_metric, filter, intervals, overlap, iteration, layer, datasplit)\u001b[0m\n\u001b[0;32m     31\u001b[0m         params={'params': f'{dataset}_{dist_metric}_{filter}_{intervals}_{overlap}', 'iteration': iteration, 'layer': layer,\n\u001b[0;32m     32\u001b[0m                 'datasplit': datasplit})\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_link_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'graph'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\probing\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    908\u001b[0m                     \u001b[1;31m# used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\probing\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\probing\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\probing\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "def induced_hierarchy(dataset, iteration, layer, datasplit, max_level=10, dist_metric='euclidean', filter='l2', intervals=50, overlap=50):\n",
    "    filename = f'images/hierarchies/{dataset}_{datasplit}_iter{iteration}_layer{layer}_level{max_level}.svg'\n",
    "\n",
    "    # if os.path.isfile(filename):\n",
    "    #     print('File exists, returning')\n",
    "    #     return\n",
    "\n",
    "    # get mapper graph\n",
    "    graph = get_graph(dataset, dist_metric, filter=filter, intervals=intervals,\n",
    "                      overlap=overlap, iteration=iteration, layer=layer, datasplit=datasplit)\n",
    "    # graph = get_graph(DATASET, 'euclidean', filter='l2', intervals=50, overlap=50, iteration=ITERATION, layer=LAYER, datasplit='train')\n",
    "\n",
    "    # get activations and labels\n",
    "    activations, labels = get_activations(dataset, iteration, layer, datasplit)\n",
    "\n",
    "    # compute distance matrix for each point\n",
    "    point_dist_mat_from_graph = compute_pointwise_distances(graph, activations, labels)\n",
    "\n",
    "    # shortest path distance using distance between node centroids as the metric\n",
    "    distance_matrix = nx.algorithms.shortest_paths.dense.floyd_warshall_numpy(graph, weight='centroid_dist')\n",
    "\n",
    "    # set disconnected node distance to max + euclidean distance\n",
    "    # max_distance = np.ma.masked_invalid(distance_matrix).max()\n",
    "    max_distance = pdist(activations.iloc[np.random.randint(activations.shape[0], size=500), :]).max() + 5\n",
    "    nodelist = list(graph.nodes())\n",
    "\n",
    "    for i in range(len(distance_matrix)):\n",
    "        for j in range(len(distance_matrix)):\n",
    "            if np.isinf(distance_matrix[i][j]):\n",
    "                centroid_i = np.array(graph.nodes[nodelist[i]]['membership']['centroid'])\n",
    "                centroid_j = np.array(graph.nodes[nodelist[j]]['membership']['centroid'])\n",
    "                distance_matrix[i][j] = max_distance + np.linalg.norm(centroid_i - centroid_j)\n",
    "\n",
    "    # return distance_matrix\n",
    "\n",
    "    # convert node-to-node distance matrix to point-to-node distance matrix\n",
    "    ptn_dict = point_to_node(graph.nodes)\n",
    "    point_dist_mat_from_graph = node_to_point_matrix(activations, ptn_dict, distance_matrix)\n",
    "\n",
    "    # Perform hierarchical clustering using pointwise distance matrix\n",
    "    model_aggclust_mapper = AgglomerativeClustering(n_clusters=None, distance_threshold=99, affinity='precomputed', linkage='average')\n",
    "    model_aggclust_mapper.fit(point_dist_mat_from_graph)\n",
    "\n",
    "    # compute the linkage matrix\n",
    "    linkage_matrix_mapper = linkage_matrix(model_aggclust_mapper)\n",
    "    tree = to_tree(linkage_matrix_mapper)\n",
    "    populate_tree_labels_counter(tree, labels)\n",
    "\n",
    "    # Create a networkx directional graph\n",
    "    linkage_graph = nx.DiGraph()\n",
    "\n",
    "    # Populate the graph with using BFS traversal of the tree\n",
    "    bfs_traversal(tree, linkage_graph, max_level=max_level)\n",
    "\n",
    "    # Save SVG file of the graph\n",
    "    Ag = nx.nx_agraph.to_agraph(linkage_graph)\n",
    "    Ag.layout(prog='neato')\n",
    "    Ag.draw(filename, format='svg', args=\"-Nshape=box\")\n",
    "\n",
    "    return linkage_graph\n",
    "\n",
    "\n",
    "# # Train Layer 12\n",
    "g = induced_hierarchy('ss-role', iteration=175, layer=12, datasplit='train', max_level=10)\n",
    "# induced_hierarchy('ss-role', iteration=5, layer=12, datasplit='train', max_level=50)\n",
    "\n",
    "# # Train Layer 0\n",
    "# induced_hierarchy('ss-role', iteration=175, layer=0, datasplit='train', max_level=50)\n",
    "# induced_hierarchy('ss-role', iteration=5, layer=0, datasplit='train', max_level=50)\n",
    "\n",
    "# # Test Layer 12\n",
    "# induced_hierarchy('ss-role', iteration=175, layer=12, datasplit='test', max_level=50)\n",
    "# induced_hierarchy('ss-role', iteration=5, layer=12, datasplit='test', max_level=50)\n",
    "\n",
    "# # Test Layer 0\n",
    "# induced_hierarchy('ss-role', iteration=175, layer=0, datasplit='test', max_level=50)\n",
    "# induced_hierarchy('ss-role', iteration=5, layer=0, datasplit='test', max_level=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_list = [5, 65, 175]\n",
    "layer_list = [0, 6, 9, 12]\n",
    "datasplit_list = ['train', 'test']\n",
    "\n",
    "for iteration in iteration_list:\n",
    "    for layer in layer_list:\n",
    "        for datasplit in datasplit_list:\n",
    "            induced_hierarchy('ss-role', iteration=iteration, layer=layer, datasplit=datasplit, max_level=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(distance_threshold=50, linkage='average',\n",
       "                        n_clusters=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_aggclust_mapper = AgglomerativeClustering(n_clusters=None, distance_threshold=50, affinity='precomputed', linkage='average')\n",
    "# model_aggclust_mapper.fit(point_dist_mat_from_graph)\n",
    "\n",
    "# model_aggclust_act = AgglomerativeClustering(n_clusters=None, distance_threshold=50, linkage='average')\n",
    "# model_aggclust_act.fit(activations)\n",
    "\n",
    "# model_dbscan_act = DBSCAN(eps=elbow_eps(activations), min_samples=1)\n",
    "# model_dbscan_act.fit(activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dbscan_act' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-1f73655ed05a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mplot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dbscan_act\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_dbscan_act' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_data(data, color):\n",
    "    xy = pd.DataFrame(PCA(n_components=2).fit_transform(activations), columns=['x', 'y'])\n",
    "\n",
    "    # plot scatterplot using seaborn\n",
    "    sns.scatterplot(data=xy, x='x', y='y', hue=color, s=100, alpha=0.5)\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "plot_data(activations, pd.Series(model_dbscan_act.labels_).astype(str))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc5676d886a2778973ff3e34aec4cad8965d07457120715c016f8e27d3c13f9b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('probing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
