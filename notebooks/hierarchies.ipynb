{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, to_tree\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import kneed\n",
    "from collections import deque\n",
    "\n",
    "import sys, os\n",
    "\n",
    "sys.setrecursionlimit(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(path, dataset='ss-role'):\n",
    "    label_data = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if dataset in ['ss-func', 'ss-role']:\n",
    "                word_info, word_label = line.strip().split('\\t')\n",
    "                sent_info, word = word_info.split(':')\n",
    "                sent_info = ast.literal_eval(sent_info)\n",
    "                label_data.append([sent_info[0], sent_info[1], word, word_label])\n",
    "            elif dataset == 'dep':\n",
    "                word_pair, word_label = line.strip().split('\\t')\n",
    "                word_pair = word_pair.split('--')\n",
    "                if len(word_pair) == 2:\n",
    "                    word1, word2 = word_pair\n",
    "                elif len(word_pair) == 3:\n",
    "                    word1, word2 = word_pair[0], '--'\n",
    "                sent_info = [0, 0]\n",
    "                label_data.append([sent_info[0], sent_info[1], f'{word1}--{word2}', word_label])\n",
    "            else:\n",
    "                raise ValueError('Dataset not supported')\n",
    "\n",
    "    return pd.DataFrame(label_data, columns=['sent_id', 'word_id', 'word', 'label'])\n",
    "\n",
    "\n",
    "def get_graph(dataset, dist_metric, filter, intervals, overlap, iteration, layer, datasplit):\n",
    "    # make request to local server at port 5000 at \\graph\n",
    "    # with the query: {params: 'ss-role_euclidean_l2_50_50', iteration: 0, layer: 12, datasplit: 'train'}\n",
    "    # and save the response as a variable\n",
    "    r = requests.get(\n",
    "        'http://localhost:5000/graph',\n",
    "        params={'params': f'{dataset}_{dist_metric}_{filter}_{intervals}_{overlap}', 'iteration': iteration, 'layer': layer,\n",
    "                'datasplit': datasplit})\n",
    "    data = r.json()\n",
    "    graph = nx.json_graph.node_link_graph(data['graph'])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_activations(dataset, iteration, layer, datasplit):\n",
    "    activations = pd.read_csv(\n",
    "        f'../backend/data/{dataset}/fine-tuned-bert-base-uncased/{datasplit}/{iteration}/{layer}.txt', delim_whitespace=True, header=None)\n",
    "    labels = read_labels(f'../backend/data/{dataset}/entities/{datasplit}.txt', dataset=dataset)\n",
    "\n",
    "    return activations, labels\n",
    "\n",
    "\n",
    "def point_to_node(nodes, num_points=4282):\n",
    "    ptn_dict = {}\n",
    "    for point_idx in range(num_points):\n",
    "        for node_idx, node in enumerate(nodes):\n",
    "            node_data = nodes[node]['membership']['membership_ids']\n",
    "\n",
    "            if point_idx in node_data:\n",
    "                ptn_dict[point_idx] = node_idx\n",
    "\n",
    "    return ptn_dict\n",
    "\n",
    "\n",
    "def linkage_matrix(model):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "\n",
    "    return linkage_matrix\n",
    "\n",
    "\n",
    "def plot_dendrogram(model):\n",
    "    linkage_mat = linkage_matrix(model)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    return dendrogram(linkage_mat, orientation='left', truncate_mode='level', p=8, leaf_font_size=12, leaf_label_func=leaf_label)\n",
    "\n",
    "\n",
    "def leaf_label(idx):\n",
    "    if idx < len(labels):\n",
    "        return labels.iloc[idx]['label']\n",
    "    else:\n",
    "        return 'None'\n",
    "\n",
    "\n",
    "def elbow_eps(data):\n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    distances = np.sort(distances, axis=0)[::-1]\n",
    "    kneedle = kneed.KneeLocator(distances[:, 1], np.linspace(0, 1, num=len(distances)), curve='convex', direction='decreasing')\n",
    "    eps = kneedle.knee * 0.75\n",
    "    return eps\n",
    "\n",
    "\n",
    "def populate_tree_labels(treeNode, labels):\n",
    "    if treeNode.id < len(labels):\n",
    "        treeNode.label = set([labels.iloc[treeNode.id]['label'].split('.')[1]])\n",
    "    else:\n",
    "        # popluate children\n",
    "        populate_tree_labels(treeNode.left, labels)\n",
    "        populate_tree_labels(treeNode.right, labels)\n",
    "\n",
    "        # label is union of labels of left and right children\n",
    "        treeNode.label = treeNode.left.label | treeNode.right.label\n",
    "\n",
    "    return treeNode\n",
    "\n",
    "\n",
    "def process_label(label, max_length=15):\n",
    "    label_str = ','.join(sorted(label))\n",
    "    if len(label_str) > max_length:\n",
    "        return f'{label_str[:max_length]}... ({len(label)})'\n",
    "    else:\n",
    "        return label_str\n",
    "\n",
    "\n",
    "def bfs_traversal(treeNode, graph, max_level=5):\n",
    "    q = deque()\n",
    "\n",
    "    q.append(treeNode)\n",
    "    level = 0\n",
    "\n",
    "    while len(q) > 0 and level < max_level:\n",
    "        level_size = len(q)\n",
    "\n",
    "        for _ in range(level_size):\n",
    "            node = q.popleft()\n",
    "            num_labels = len(node.label)\n",
    "\n",
    "            graph.add_node(node.id, label=process_label(node.label))\n",
    "\n",
    "            # terminate if node's label has one label\n",
    "            if num_labels > 1:\n",
    "                if node.left is not None:\n",
    "                    q.append(node.left)\n",
    "                    graph.add_edge(node.id, node.left.id, weight=node.left.dist)\n",
    "                    graph.add_node(node.left.id, label=process_label(node.left.label))\n",
    "\n",
    "\n",
    "                if node.right is not None:\n",
    "                    q.append(node.right)\n",
    "                    graph.add_edge(node.id, node.right.id, weight=node.right.dist)\n",
    "                    graph.add_node(node.right.id, label=process_label(node.right.label))\n",
    "\n",
    "        level += 1\n",
    "\n",
    "\n",
    "def node_to_point_matrix(activations, ptn_dict, node_dist_matrix):\n",
    "    point_dist_mat_from_graph = np.zeros((len(activations), len(activations)))\n",
    "\n",
    "    # populate point_dist_mat_from_graph\n",
    "    for point_idx1 in range(len(activations)):\n",
    "        for point_idx2 in range(len(activations)):\n",
    "            if point_idx1 not in ptn_dict or point_idx2 not in ptn_dict:\n",
    "                point_dist_mat_from_graph[point_idx1][point_idx2] = 100\n",
    "            elif point_idx1 != point_idx2:\n",
    "                point_dist_mat_from_graph[point_idx1, point_idx2] = node_dist_matrix[ptn_dict[point_idx1], ptn_dict[point_idx2]]\n",
    "\n",
    "    return point_dist_mat_from_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def induced_hierarchy(dataset, iteration, layer, datasplit, max_level=10, dist_metric='euclidean', filter='l2', intervals=50, overlap=50):\n",
    "    filename = f'images/hierarchies/{dataset}_{datasplit}_iter{iteration}_layer{layer}_level{max_level}.svg'\n",
    "\n",
    "    # if file exists, return\n",
    "    if os.path.isfile(filename):\n",
    "        print('File exists, returning')\n",
    "        return\n",
    "        \n",
    "    # get mapper graph\n",
    "    graph = get_graph(dataset, dist_metric, filter=filter, intervals=intervals,\n",
    "                      overlap=overlap, iteration=iteration, layer=layer, datasplit=datasplit)\n",
    "    # graph = get_graph(DATASET, 'euclidean', filter='l2', intervals=50, overlap=50, iteration=ITERATION, layer=LAYER, datasplit='train')\n",
    "\n",
    "    # get activations and labels\n",
    "    activations, labels = get_activations(dataset, iteration, layer, datasplit)\n",
    "\n",
    "    # shortest path distance using distance between node centroids as the metric\n",
    "    distance_matrix = nx.algorithms.shortest_paths.dense.floyd_warshall_numpy(graph, weight='centroid_dist')\n",
    "    \n",
    "    # set disconnected node distance to max + euclidean distance\n",
    "    max_distance = np.ma.masked_invalid(distance_matrix).max()\n",
    "    nodelist = list(graph.nodes())\n",
    "\n",
    "    for i in range(len(distance_matrix)):\n",
    "        for j in range(len(distance_matrix)):\n",
    "            if np.isinf(distance_matrix[i][j]):\n",
    "                centroid_i = np.array(graph.nodes[nodelist[i]]['membership']['centroid'])\n",
    "                centroid_j = np.array(graph.nodes[nodelist[j]]['membership']['centroid'])\n",
    "                distance_matrix[i][j] = max_distance + np.linalg.norm(centroid_i - centroid_j)\n",
    "\n",
    "    # return distance_matrix\n",
    "\n",
    "    # convert node-to-node distance matrix to point-to-node distance matrix\n",
    "    ptn_dict = point_to_node(graph.nodes)\n",
    "    point_dist_mat_from_graph = node_to_point_matrix(activations, ptn_dict, distance_matrix)\n",
    "\n",
    "    # Perform hierarchical clustering using pointwise distance matrix\n",
    "    model_aggclust_mapper = AgglomerativeClustering(n_clusters=None, distance_threshold=99, affinity='precomputed', linkage='average')\n",
    "    model_aggclust_mapper.fit(point_dist_mat_from_graph)\n",
    "\n",
    "    # compute the linkage matrix\n",
    "    linkage_matrix_mapper = linkage_matrix(model_aggclust_mapper)\n",
    "    tree = to_tree(linkage_matrix_mapper)\n",
    "    populate_tree_labels(tree, labels)\n",
    "\n",
    "    # Create a networkx directional graph\n",
    "    linkage_graph = nx.DiGraph()\n",
    "\n",
    "    # Populate the graph with using BFS traversal of the tree\n",
    "    bfs_traversal(tree, linkage_graph, max_level=max_level)\n",
    "\n",
    "    # Save SVG file of the graph\n",
    "    Ag = nx.nx_agraph.to_agraph(linkage_graph)\n",
    "    Ag.layout(prog='neato')\n",
    "    Ag.draw(filename, format='svg', args=\"-Nshape=box\")\n",
    "\n",
    "    return linkage_graph\n",
    "\n",
    "\n",
    "# # Train Layer 12\n",
    "# induced_hierarchy('ss-role', iteration=175, layer=12, datasplit='train', max_level=50)\n",
    "# induced_hierarchy('ss-role', iteration=5, layer=12, datasplit='train', max_level=50)\n",
    "\n",
    "# # Train Layer 0\n",
    "# induced_hierarchy('ss-role', iteration=175, layer=0, datasplit='train', max_level=50)\n",
    "# induced_hierarchy('ss-role', iteration=5, layer=0, datasplit='train', max_level=50)\n",
    "\n",
    "# # Test Layer 12\n",
    "# induced_hierarchy('ss-role', iteration=175, layer=12, datasplit='test', max_level=50)\n",
    "# induced_hierarchy('ss-role', iteration=5, layer=12, datasplit='test', max_level=50)\n",
    "\n",
    "# # Test Layer 0\n",
    "# induced_hierarchy('ss-role', iteration=175, layer=0, datasplit='test', max_level=50)\n",
    "# induced_hierarchy('ss-role', iteration=5, layer=0, datasplit='test', max_level=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_list = [5, 65, 175]\n",
    "layer_list = [0, 6, 9, 12]\n",
    "datasplit_list = ['train', 'test']\n",
    "\n",
    "for iteration in iteration_list:\n",
    "    for layer in layer_list:\n",
    "        for datasplit in datasplit_list:\n",
    "            induced_hierarchy('ss-role', iteration=iteration, layer=layer, datasplit=datasplit, max_level=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(distance_threshold=50, linkage='average',\n",
       "                        n_clusters=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_aggclust_mapper = AgglomerativeClustering(n_clusters=None, distance_threshold=50, affinity='precomputed', linkage='average')\n",
    "# model_aggclust_mapper.fit(point_dist_mat_from_graph)\n",
    "\n",
    "# model_aggclust_act = AgglomerativeClustering(n_clusters=None, distance_threshold=50, linkage='average')\n",
    "# model_aggclust_act.fit(activations)\n",
    "\n",
    "# model_dbscan_act = DBSCAN(eps=elbow_eps(activations), min_samples=1)\n",
    "# model_dbscan_act.fit(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dbscan_act' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-1f73655ed05a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mplot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dbscan_act\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_dbscan_act' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_data(data, color):\n",
    "    xy = pd.DataFrame(PCA(n_components=2).fit_transform(activations), columns=['x', 'y'])\n",
    "\n",
    "    # plot scatterplot using seaborn\n",
    "    sns.scatterplot(data=xy, x='x', y='y', hue=color, s=100, alpha=0.5)\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "plot_data(activations, pd.Series(model_dbscan_act.labels_).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting summary\n",
    "- Ran hierarchy extraction algorithm and visualization \n",
    "  - Using a modified version of the shortest path distance in the mapper graph compute the distance matrix for the points\n",
    "  - [TODO] Replace max_distance with diameter (in euclidean sense) of activation space\n",
    "    - [TODO] First check if diameter is larger than max tree distance, otherwise \n",
    "- [TODO] Show class distributions in the leaves instead of just distinct class names\n",
    "- [TODO] Also add counts of labels to the nodes, and sort by majority classes\n",
    "- [TODO] Show purity trends in induced hierarchy in epoch 0 vs epoch 175\n",
    "  - x-axis: distance from root node, y-axis: purity distribution\n",
    "- Human hierarchy and neural network derived topological hierarchy places where they align and misalign\n",
    "  - Machine is unaware of human curated hierarchy - aligments make sense, but mismatches are interesting too\n",
    "    - Reason for mismatch: shared words between categories? Improper formulation of metric? \n",
    "- Attaching the hierarchy SVGs for train data through layer 12, epoch 175 \n",
    "\n",
    "Cross-task activations\n",
    "- No major deviation from our hypothesis: Role And Func activation behave similarly, POS ones do not have much label purity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find maximim pairwise distance of activations\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc5676d886a2778973ff3e34aec4cad8965d07457120715c016f8e27d3c13f9b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('probing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
